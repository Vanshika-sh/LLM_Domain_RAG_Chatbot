Metadata-Version: 2.1
Name: trulens-eval
Version: 0.28.2
Summary: Library to systematically track and evaluate LLM based applications.
Home-page: https://www.trulens.org
Author: Truera Inc
Author-email: all@truera.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >= 3.8, < 3.13
Description-Content-Type: text/markdown
Requires-Dist: numpy >=1.23.5
Requires-Dist: frozendict >=2.3.8
Requires-Dist: munch >=3.0.0
Requires-Dist: dill >=0.3.7
Requires-Dist: tqdm >=4.66.1
Requires-Dist: nltk >=3.8.1
Requires-Dist: requests >=2.31.0
Requires-Dist: nest-asyncio >=1.5.8
Requires-Dist: typing-extensions >=4.9.0
Requires-Dist: psutil >=5.9.8
Requires-Dist: pip >=24.0
Requires-Dist: packaging >=23.2
Requires-Dist: python-dotenv >=1.0.0
Requires-Dist: pydantic <3,>=2
Requires-Dist: merkle-json >=1.0.0
Requires-Dist: langchain >=0.1.14
Requires-Dist: langchain-core >=0.1.6
Requires-Dist: typing-inspect >=0.8.0
Requires-Dist: millify >=0.1.1
Requires-Dist: humanize >=4.6.0
Requires-Dist: streamlit >=1.32.2
Requires-Dist: streamlit-aggrid ==0.3.4
Requires-Dist: streamlit-extras >=0.4.0
Requires-Dist: streamlit-pills >=0.3.0
Requires-Dist: rich >=13.6.0
Requires-Dist: sqlalchemy >=2.0.19
Requires-Dist: alembic >=1.11.2

<!---
start of docs/trulens_eval/intro.md
NOTE: This content is from docs/trulens_eval/intro.md and is merged into
trulens_eval/README.md . If you are editing README.md, your changes will be overwritten.
-->
# Welcome to TruLens-Eval!

![TruLens](https://www.trulens.org/assets/images/Neural_Network_Explainability.png)

**Don't just vibe-check your llm app!** Systematically evaluate and track your
LLM experiments with TruLens. As you develop your app including prompts, models,
retreivers, knowledge sources and more, *TruLens-Eval* is the tool you need to
understand its performance.

Fine-grained, stack-agnostic instrumentation and comprehensive evaluations help
you to identify failure modes & systematically iterate to improve your
application.

Read more about the core concepts behind TruLens including [Feedback
Functions](https://www.trulens.org/trulens_eval/getting_started/core_concepts/
[The RAG Triad](https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/),
and [Honest, Harmless and Helpful
Evals](https://www.trulens.org/trulens_eval/getting_started/core_concepts/honest_harmless_helpful_evals/).

## TruLens in the development workflow

Build your first prototype then connect instrumentation and logging with
TruLens. Decide what feedbacks you need, and specify them with TruLens to run
alongside your app. Then iterate and compare versions of your app in an
easy-to-use user interface ðŸ‘‡

![Architecture
Diagram](https://www.trulens.org/assets/images/TruLens_Architecture.png)

## Installation and Setup

Install the trulens-eval pip package from PyPI.

```bash
    pip install trulens-eval
```

## Quick Usage

Walk through how to instrument and evaluate a RAG built from scratch with
TruLens.

[![Open In
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/quickstart.ipynb)

### ðŸ’¡ Contributing

Interested in contributing? See our [contributing
guide](https://www.trulens.org/trulens_eval/contributing/) for more details.
<!---
end of docs/trulens_eval/intro.md
-->
